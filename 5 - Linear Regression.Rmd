---
title: "Lesson 5 - Linear Regresssion"
author: "ZJ"
date: "16 October, 2014"
output: ioslides_presentation
---

## Regression

Linear Regression

- Finding the **line** of best fit?

## The data

Use the animal dataset

- body the body mass of a list of animals
- brain the brain szie of a list of animals
```{r}
animal <- read.csv("Animals2.csv")
head(animal)
```

## First step - inspect the data

```{r,message=FALSE}
require(dplyr) # the 'select' function is from dplyr
plot(select(animal,brain,body)) # select is from dplyr
```

## First step - inspect the data

Remove outliers
```{r}
filter(animal, brain > 3000)
filter(animal, body > 8000)

```

## First step - inspect the data

```{r}
animal.1 <- filter(animal, brain <= 3000, body <=8000)
plot(select(animal.1,body,brain))
```

## First step - inspect the data

```{r}
filter(animal.1, brain >= 1200)
animal.2 <- filter(animal.1, X != "Human")
```

## First step - inspect the data

```{r}
plot(select(animal.2,body,brain))
```

## Modelling - Linear Regression

- The **lm** function is used for linear regression
- lm stands for linear models
- $$ y = a + bx + e $$
- e is the error
```{r}
m <- lm(brain ~ body, data = animal.2)
```

## Modelling - Linear Regression {.smaller}

```{r}
summary(m)
```

## What to look for?

- The coefficients
- The p-value of coefficients
  - a test was performed to make sure that the coefficients is statistically significant
  - the lower the p-value the more statistically signficiant it is
  - a 5% threshold is usually used
- R-squared - a measure of the proportion of variation in the data explained by the model

## What to look for? {.smaller}

```{r}
summary(m)
```

## Modelling - Linear Regression

```{r}
plot(select(animal.2,body, brain))
abline(m)
```

## Modelling - Linear Regression {.smaller}

```{r}
m <- lm(log(brain) ~ log(body), data = animal.2)
summary(m)
```

## How to assess linear regression models?

- Check if the coefficients are significant (p-val < 0.05)
- Check the R-square
  - It represent the proportion of variance in the data explained by the model. It is a number from 0 to 1.00. The higher the better
- Assess the data visually
- Try a few transformations of the raw data and assess

## Exercise

- Basic form of linear model
  - m <- lm(dependent ~ column1 + column2, data = some.data.frame)
  - depend is the column that you wish to build a model to predict
- abline(m) will plot the model
- Using the cars data.frame Build a linear model to predict "dist" the stopping distance using speed
- Explain the result
- install.packages("nutshell")

## Importance of visual inspection

```{r}
m <- lm(dist ~ speed , data=cars)
plot(select(cars,speed,dist))
abline(m)
```

## Importance of visual inspection

A single point, that is far away from the other points can cause big swing in results
- these points can be called outliers, but another name is leverage points
```{r}
cars1 <- rbind(cars,c(100,200))
lm(dist ~ speed, data = cars1)
```

## Importance of visual inspection

A single point, that is far away from the other points can cause big swing in results
- these points can be called outliers, but another name is leverage points
```{r}
plot(cars1)
```

## Multipe Linear Regression

Multiple explanatory variable
$y = a + b_1x_1 + b_2x_2 + ... + b_nx_n$
where x_n are the data and a the intercept and b_i's the coefficients

```{r}
#install.packages("nutshell")
library(nutshell)
data(team.batting.00to08) # this makes the data.frame team.batting.00to08 available
names(team.batting.00to08)
runs.mdl <- lm(runs ~ singles + doubles, data = team.batting.00to08)
```

## Multipe Linear Regression {.smaller}

```{r}
summary(runs.mdl)
```

## Things to watch out for

$$ y = a + 2x_1 + 3x_2 $$

- The intepretation is: *holding everything constant* for every unit increase in x_1, y increases by 2,
- What if x_1 and x_2 are highly correlated?
- Highly correlated data can cause **multicollinearity**
- means the coefficients has high variability
- try removing 10% of your data and refit and see how much the coefficients changes