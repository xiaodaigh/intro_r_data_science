---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Fitting a XGBoost model

1st step - prepare your dataset
```{r include=FALSE}
library(data.table)
library(disk.frame)
```


```{r echo=TRUE}
df = fread("data/cs-training.csv")
df_column = df[,-"SeriousDlqin2yrs", with = F]
df_label = df[,SeriousDlqin2yrs == 1]
```

2nd step - convert your R data.frame to `xgb.DMatrix` format
```{r echo=TRUE}
xdm = xgboost::xgb.DMatrix(
  label = df_label, # the thing you are trying to predict
  data = as.matrix(df_column) # the features you can use to predict the label
  )
xdm
```

3rd step - fitting the model
```{r echo=TRUE}
xgm = xgboost::xgboost(
  data=xdm,
  nrounds = 1, # number of trees to build
  max_depth = 3, # number of layers in the tree
  tree_method="exact", # go through this algorithm; use this when data set is small
  objective = "binary:logitraw"
  )
xgm
```

So the probability of
```{r}
xgm = xgboost::xgboost(
  data=xdm,
  nrounds = 2, # number of trees to build
  max_depth = 3,
  tree_method="exact",
  objective = "binary:logitraw"
  #,base_score = df[,sum(SeriousDlqin2yrs)/.N]
  )

xgmdt  = xgboost::xgb.model.dt.tree(model = xgm)
xgmdt[Feature == "Leaf", prob := 1/(1 + exp(-Quality))]
xgmdt
```

```{r echo=FALSE}
df[, value := predict(xgm, xdm)]
df[, value1 := predict(xgm, xdm, outputmargin = T)]
df[, leaf_index := predict(xgm, xdm, predleaf = T)]
#disk.frame::auc(df$SeriousDlqin2yrs, df$value)
df[, .(value, value1, leaf_index)]
```

```{r}
df_summ = df[,.(actual = sum(SeriousDlqin2yrs)/.N, value = mean(value), min=min(value), max = max(value)), leaf_index]
df_summ[, prob := 1/(1+exp(-value))]
df_summ
```

```{r}
plot(df_summ[,.(actual, prob)], ylim = c(0,1), xlab = "Actual Default Rate", ylab = "Predicted Default Rate", main="Actual vs Predicted Default Rate (base_score = 0.5)")
abline(a=0,b=1)
```


You can fit the model by supplying a `base_score`
```{r echo=TRUE}
xgm = xgboost::xgboost(
  data=xdm,
  nrounds = 1, # number of trees to build
  max_depth = 3,
  tree_method="exact",
  objective = "binary:logitraw"
  ,base_score = df[,sum(SeriousDlqin2yrs)/.N]
  )

df[, value := predict(xgm, xdm)]
df[, value1 := predict(xgm, xdm, outputmargin = T)]
df[, leaf_index := predict(xgm, xdm, predleaf = T)]
df_summ = df[,.(actual = sum(SeriousDlqin2yrs)/.N, value = mean(value)), leaf_index]
df_summ[, prob := 1/(1+exp(-value))]
```


Plotting result
```{r echo=FALSE}
plot(df_summ[,.(actual, prob)], ylim = c(0,1), xlab = "Actual Default Rate", ylab = "Predicted Default Rate", main="Actual vs Predicted Default Rate (base_score = 0.066)")
abline(a=0,b=1)
```


## What is AUC?

We often do not assess the model on accurary but rather of ranking ability. Which is measured by AUC

AUC = area under the curve?

**What is the curve?**

```{r}
df[,random_value := runif(.N)]
setkey(df, random_value)
df[,.(SeriousDlqin2yrs, random_value)]
```

Plot of cumulative %defaults vs %total
```{r}
dfc = df[,.(cum_tot = (1:.N)/.N , cum_defaults = cumsum(SeriousDlqin2yrs)/sum(SeriousDlqin2yrs))]
dfc
```

```{r}
plot(dfc, main="AUC with random prediction", col="red", type="l")
abline(a=0, b=1, col="black", lty=2)
legend("topleft", c("Random Model", "Straight line y = x"), col=c("red","black"), lty=c(1,2))
```



Plot of cumulative %defaults vs %total
```{r}
df[,neg_value := -value]
setkey(df, neg_value)
dfc_value = df[,.(cum_tot = (1:.N)/.N , cum_defaults = cumsum(SeriousDlqin2yrs)/sum(SeriousDlqin2yrs))]
dfc_value
```

```{r}
plot(dfc_value, main="AUC with random prediction", type="l", col= "blue")
lines(dfc, col = "red")
legend("topleft", c("XGBoost Model", "Random Model"), lty=1, col=c("blue","red"))
```