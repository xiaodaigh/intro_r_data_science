Lesson 5 - Classic Data Science Method
========================================================
author: Dai ZJ
width: 1650
height: 1050
```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

Classic Algorithms
========================================================
Bread of butter
- Linear Regression (lm)
- K-means algorithms (kmeans)
- Logistic Regression (glm)
- Decision Tree (rpart)

Kmeans
========================================================
Finding clusters of points by locating the **centre** point of clusters
- Barclays Premiere League (BPL)
  - Twenty teams play each other 2 times per season
  - Team earn 3 points of each win, 1 for a draw and 0 for a loss
  - The team with most points win
- Can we identify the number of tiers of teams?

BPL
========================================================
```{r}
bpl <- read.csv("bpl.csv")
bpl
# compute the points for each team
require(dplyr)
bpl <- mutate(bpl, pts = W*3 + D)
head(bpl)
```

Kmeans
========================================================
The kmeans
```{r}
# number of unique pts
# the number of groups cannot be greater than the number of point
fit <- kmeans(bpl$pts, 6)
plot(bpl$pts,y=rep(0,nrow(bpl)), col = fit$cluster)
points(fit$centers, rep(0,length(fit$centers)), pch = 21, bg = "black")
bpl$cluster <- fit$cluster
```

How many clusters?
========================================================
- Often choosing the number of clusters is more art than science
- How good a clustering is measured by the sum within cluster sum of squares (withinss)
- The (Euclidean) distance squared from the members of the clusters to the centre points
- withinss is good for comparing two competing clusters with the same k
- but not designed to compare clustering with the different number of clusterings
```{r}
fit$withinss
fit$tot.withinss
```

How many clusters - elbow approach
========================================================
- Run kmeans based on 2 to as many clusters as you like
- plot the withinss by number of k for all the kmeans runs
- look for the elbow in the plot
```{r}
tot.withinss <- NULL
for(i in 2:length(unique(bpl$pts))) {
  fit <- kmeans(bpl$pts, i)
  tot.withinss <- c(tot.withinss, fit$tot.withinss)
}
plot(2:length(unique(bpl$pts)), tot.withinss, type="b")
# arguably there is not much improvement after 5 clusters
# you can see clearly very big improvements from 
```

Results check
========================================================
- Look at the results
```{r}
fit <- kmeans(bpl$pts, 5)
bpl$cluster <- fit$cluster
```

Kmeans - Multidimensional
========================================================
- Let's analysis the Iris dataset again
- We know there are 3 species
```{r}
fit.iris <- kmeans(select(iris,Sepal.Length:Petal.Width), 3)
fit.iris
```

Kmeans - Multidimensional
========================================================
- Let's see how it did
```{r}
iris.cluster <- mutate(iris, cluster = fit.iris$cluster)
table(iris.cluster[,c("Species","cluster")])
```