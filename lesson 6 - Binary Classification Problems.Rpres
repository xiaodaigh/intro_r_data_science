Lesson 3 - Logistic Regression
========================================================
author: Dai ZJ
transition: rotate
#width: 1650
#height: 1050

```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

Read in the data
========================================================
```{r}
# read in the Kaggle sampled data
# from the Kaggle give me some credit competition
training <- read.csv("~/Dropbox/AnalytixWare/Data/Kaggle/cs-training-sample.csv")
nrow(training)
```

The data
========================================================
SeriousDlqin2yrs - 1 means the customer was in serious delinquency
Terminology
- Bad - customer has serious delinquency
- Good - customer does not have serious delinquency
Try to build a model that predicts that

Simple Data Exploration
========================================================
```{r}
# give a frequency count of the unique values
table(training$SeriousDlqin2yrs)

# 968 customers has defaulted 
# 14032 has not
# want to build a modle that can predict who is more likley to be in the 1 (defaulted) group

names(training)
```

Simple plot
========================================================
```{r}
# you can't tell anything
# definitely not the right way to visualise this type of data
plot(training$age, training$SeriousDlqin2yrs)
```

Default Rate
========================================================
```{r}
freq <- table(training$age, training$SeriousDlqin2yrs)
head(freq)
gb.odds <- freq[,1] / freq[,2]
# dr stands for default rate
dr <- freq[,2] / (freq[,1] + freq[,2])
```

Default Rate
========================================================
```{r}
# plot the Good/Bad Odds (odds ratio)
plot(dr)
```

Default Rate
========================================================
```{r}
# plot the Good/Bad Odds (odds ratio)
plot(sort(unique(training$age)), dr)

#lines(lowess(dr))
```

What about for MonthlyIncome (1)
========================================================
```{r}
freq <- table(training$MonthlyIncome, training$SeriousDlqin2yrs)
dr <- freq[,1] / (freq[,1] + freq[,2])
plot(dr)
```


What about for MonthlyIncome (2)
========================================================
```{r}
plot(dr)
```

pretty & cut (1)
========================================================
```{r}
cut.points <- pretty(training$MonthlyIncome)
cut.points
MonthlyIncome.cut <- cut(training$MonthlyIncome, cut.points)
table(MonthlyIncome.cut)
```

pretty & cut (2)
========================================================
```{r}
require(dplyr)
cut.points <- pretty(filter(training, MonthlyIncome < 25000)$MonthlyIncome)
cut.points
MonthlyIncome.cut <- cut(training$MonthlyIncome, c(-Inf,cut.points, Inf))
table(MonthlyIncome.cut)
```

pretty & cut (2)
========================================================
```{r}
freq <- table(MonthlyIncome.cut, training$SeriousDlqin2yrs)
dr <- freq[,2] / (freq[,1] + freq[,2])
plot(dr, type = "b")
```

Summary
========================================================
- Use ratios for binary outcome data
- Grouping data into bins can be an effective strategy (pretty & cut)

Exercise
========================================================
- Explore the documentation for pretty (e.g. type ?pretty into the console)
- Cut the MonthlyIncome into roughly 10 cutpoints instead
- plot the default rate. Is there a clear trend?
- How do you explain where income goes up but default rate goes down?

Exercise (Solution)
========================================================
```{r}
cut.points.10 <- pretty(filter(training,MonthlyIncome<25000)$MonthlyIncome, n= 10)
Monthly.Income.cut10 <- cut(training$MonthlyIncome, c(-Inf,cut.points.10,Inf))
freq <- table(Monthly.Income.cut10, training$SeriousDlqin2yrs)
dr <- freq[,2] / (freq[,1] + freq[,2])
plot(dr)
```

Can we group the data into bins with an unbroken DR% trends?
========================================================
Yes. Let's implement a simple monotone classifier (maximum likelihood monotone classifier)
```{r}
# source willl run the source code in another file
source("mlmc.r")
binning <- mlmc(training$MonthlyIncome, training$SeriousDlqin2yrs)
```

Exercise
========================================================
- Use the mlmc function to create binnings for NumberOfDependents. Explain the resutls
- Use the mlmc function to create binnings for -NumberOfDependents (i.e. negative Number of Dependents). Explain the results what can you say about the MLMC algorithm

Exercise (Solution)
========================================================
```{r}
# source willl run the source code in another file
source("mlmc.r")
binning <- mlmc(training$NumberOfDependents, training$SeriousDlqin2yrs)
binning.neg <- mlmc(-training$NumberOfDependents, training$SeriousDlqin2yrs)
```

Logistic Regression Model
========================================================
- Perfect for binary outcome data
- p is probability of non-default
- $$ \log(\frac{p}{1-p}) =  \alpha_0 + \alpha_1 \times x_1 $$
- Use the binning approach

Logistic Regression Model
========================================================
- Perfect for binary outcome data
- p is probability of non-default
- $$ \log(\frac{p}{1-p}) =  \alpha_0 + \alpha_1 \times x_1 $$
- Use the binning approach

Logistic Regression Model
========================================================
Weight of evidence (WOE) transformation
- commonly used in banking
```{r}
# create woe transformation from raw data and provided cut points (cp)
woe <- function(raw, default, cp) {
  raw.cut <- addNA(cut(raw,c(-Inf,cp,Inf)))
  freq <- table(raw.cut, default)
  tot.b <- sum(default)
  tot.g <- length(default) - tot.b
  woe <- log(freq[,1] / freq[,2]) - log(tot.g / tot.b)
  woe[raw.cut]
}

mlmc.mi <- mlmc(training$MonthlyIncome, training$SeriousDlqin2yrs)
woe.mi <- woe(training$MonthlyIncome, training$SeriousDlqin2yrs, mlmc.mi$cp)
```

Logistic Regression Model
========================================================
```{r}
m <- glm(relevel(as.factor(SeriousDlqin2yrs),ref="1") ~ woe.mi, data=training, family = binomial)
summary(m)
```

How to intepret the coefficients?
========================================================
- Intercept = a = 2.67386
- The average portfolio log(Good Bad Odds) is 2.67386
- $ln(p/(1-p)) = a + bx$ where p is probability of good
- ln(Prob(Good) / Prob(Bad)) = 2.67386 + 1 * WOE.Monthly Income
- woe = ln(Goods/Bads) - ln(overall Goods/Overall Bad)

Logistic Regression Model
========================================================
Weight of evidence (WOE) transformation
- commonly used in banking
```{r}
# create woe transformation from raw data and provided cut points (cp)
mlmc.age <- mlmc(training$age, training$SeriousDlqin2yrs)
woe.age <- woe(training$age, training$SeriousDlqin2yrs, mlmc.mi$cp)
# collapse last two bins
l <- length(mlmc.age$cp)
woe.age <- woe(training$age, training$SeriousDlqin2yrs, mlmc.age$cp[-l])
table(cut(training$age,c(-Inf,mlmc.age$cp[-l],Inf)), training$SeriousDlqin2yrs)
```

Exercise 
========================================================
- Create the woe version of all the varibles in the dataset
- Fit a logistics regression

Area under the curve
========================================================
How to assess the performance?
- AIC - lower the better, can be used to compare differnt models using the same data
- Area under the curve
  - also called GINI, accuracy ratio (AR)
```{r}
#install.packages("pROC")
require(pROC)
g <- roc(SeriousDlqin2yrs ~ woe.mi,data=training)
```

Predict 
========================================================
Scoring out another dataset
```{r}
scores <- predict(m, training[7500:15000,])
save(m, file="model.data")
load(file="model.data")
```

Sampling
========================================================
sample(X, size)
- X is a vector
- size the number of elements to sample WITHOUT replacement from x

Random Seed
========================================================
- set.seed to set the random seed so your sampling is repeatable
```{r}
set.seed(1)
sample(1:10, 3)
sample(1:10, 3)

set.seed(1)
sample(1:10, 3)
```

Typical modelling practise
========================================================
-Sample 70% of the data to build the model (development sample)
- Use the other 30% to validate your model (holdout sample)
- If you have the luxury use an "out-of-time" sample
  - a data differnt to your model sample periods
```{r}
training.sample <- training[sample(1:nrow(training), size = nrow(training)*0.7), ]
```

Exercise
========================================================
- Follow the modelling practise
- Build a logistc model using 70% training
- Score out the 30% holdout sample
- Compute the AUC (area under curve) for both the development and holdout sampel
